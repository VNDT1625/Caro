# üöÄ ULTIMATE AI MATCH ANALYSIS IMPROVEMENT PLAN

## M·ª•c Ti√™u: ƒê·∫°t Level Engine Chuy√™n Nghi·ªáp

**Target: T·ª´ Amateur ‚Üí Semi-Pro ‚Üí Professional Level**

K·∫ø ho·∫°ch n√†y nh·∫±m n√¢ng c·∫•p AI Analysis l√™n ngang t·∫ßm v·ªõi c√°c engine Gomoku h√†ng ƒë·∫ßu nh∆∞:
- Yixin (China) - World Champion Engine
- Piskvork/Gomocup engines
- Renju Solver

---

## üìä ƒê√°nh Gi√° Hi·ªán Tr·∫°ng Chi Ti·∫øt

### ƒêi·ªÉm M·∫°nh Hi·ªán T·∫°i
- ‚úÖ Threat Detection c∆° b·∫£n (FIVE, OPEN_FOUR, FOUR, OPEN_THREE, THREE, OPEN_TWO)
- ‚úÖ Double Threat Detection (DOUBLE_FOUR, FOUR_THREE, DOUBLE_THREE)
- ‚úÖ Broken Pattern Detection (BROKEN_FOUR, BROKEN_THREE, JUMP_THREE)
- ‚úÖ VCF/VCT Search integration
- ‚úÖ Position Evaluator v·ªõi sigmoid win probability
- ‚úÖ Opening Book integration (26 Renju openings)
- ‚úÖ Endgame Analyzer integration

### üî¥ ƒêi·ªÉm Y·∫øu Nghi√™m Tr·ªçng

#### 1. Search Engine (~30% so v·ªõi engine pro)
- **Depth qu√° n√¥ng**: Ch·ªâ 1-2 ply, engine pro search 15-25 ply
- **Kh√¥ng c√≥ Alpha-Beta Pruning** ƒë√∫ng chu·∫©n
- **Kh√¥ng c√≥ Null Move Pruning**
- **Kh√¥ng c√≥ Late Move Reduction (LMR)**
- **Kh√¥ng c√≥ Principal Variation Search (PVS)**
- **Kh√¥ng c√≥ Aspiration Windows**

#### 2. Evaluation Function (~40% accuracy)
- **Thi·∫øu Neural Network evaluation** (NNUE style)
- **Kh√¥ng c√≥ Pattern Matching** v·ªõi database l·ªõn
- **Kh√¥ng c√≥ Threat Space Search**
- **Kh√¥ng c√≥ Dependency-Based Search**

#### 3. Tactical Analysis (~50% accuracy)
- **VCF search ch∆∞a optimal** - thi·∫øu pruning
- **VCT search qu√° ch·∫≠m** - kh√¥ng c√≥ heuristics
- **Kh√¥ng c√≥ Proof Number Search**
- **Kh√¥ng c√≥ Lambda Search**

#### 4. Strategic Understanding (~35% accuracy)
- **Kh√¥ng c√≥ Territory Analysis**
- **Kh√¥ng c√≥ Influence/Potential Maps**
- **Kh√¥ng c√≥ Game Phase Recognition** t·ªët
- **Kh√¥ng c√≥ Long-term Planning**

---

## üéØ TARGET METRICS (Professional Level)

| Metric | Hi·ªán t·∫°i | Target | Pro Engine |
|--------|----------|--------|------------|
| Search Depth | 2 ply | 15+ ply | 20-25 ply |
| Threat Detection | 70% | 98% | 99%+ |
| Position Evaluation | 60% | 92% | 95%+ |
| Tactical Accuracy | 50% | 95% | 98%+ |
| VCF Detection | 80% | 99% | 100% |
| VCT Detection | 60% | 95% | 98%+ |
| Best Move Accuracy | 55% | 90% | 95%+ |
| Analysis Speed | 1.5s | 0.5s | 0.1s |

---

# üî• PHASE 1: SEARCH ENGINE REVOLUTION (Tu·∫ßn 1-4)

## ƒê√¢y l√† phase quan tr·ªçng nh·∫•t - Search Engine quy·∫øt ƒë·ªãnh 60% s·ª©c m·∫°nh

### Task 1.1: Alpha-Beta v·ªõi Enhancements
**Priority: CRITICAL | Impact: +200% search efficiency**

```python
# ai/analysis/alpha_beta_search.py

class AlphaBetaSearch:
    """
    Professional-grade Alpha-Beta search v·ªõi t·∫•t c·∫£ enhancements.
    
    Features:
    - Iterative Deepening
    - Aspiration Windows
    - Principal Variation Search (PVS)
    - Null Move Pruning
    - Late Move Reduction (LMR)
    - Futility Pruning
    - Killer Move Heuristic
    - History Heuristic
    - Countermove Heuristic
    """
    
    def __init__(self, evaluator, tt_size_mb=128):
        self.evaluator = evaluator
        self.tt = TranspositionTable(tt_size_mb)
        self.killer_moves = [[None, None] for _ in range(64)]  # 2 killers per depth
        self.history = {}  # (player, from, to) -> score
        self.countermoves = {}  # prev_move -> counter_move
        self.pv_table = {}  # Principal Variation
        self.nodes_searched = 0
        
    def search(self, board, player, max_depth=15, time_limit_ms=2000):
        """
        Iterative Deepening v·ªõi Aspiration Windows.
        """
        best_move = None
        best_score = -float('inf')
        
        # Aspiration window
        alpha = -float('inf')
        beta = float('inf')
        window = 50  # Initial aspiration window
        
        for depth in range(1, max_depth + 1):
            # Check time
            if self._time_exceeded(time_limit_ms):
                break
            
            score = self._pvs_search(board, player, depth, alpha, beta, True)
            
            # Aspiration window failed - re-search with full window
            if score <= alpha or score >= beta:
                score = self._pvs_search(board, player, depth, -float('inf'), float('inf'), True)
            
            if score > best_score:
                best_score = score
                best_move = self.pv_table.get(0)
            
            # Update aspiration window
            alpha = score - window
            beta = score + window
            
        return best_move, best_score
    
    def _pvs_search(self, board, player, depth, alpha, beta, is_pv):
        """
        Principal Variation Search - t·ªëi ∆∞u h∆°n Alpha-Beta thu·∫ßn.
        """
        self.nodes_searched += 1
        
        # Transposition Table lookup
        tt_entry = self.tt.probe(board.hash, depth, alpha, beta)
        if tt_entry:
            return tt_entry.score
        
        # Terminal node
        if depth == 0:
            return self._quiescence_search(board, player, alpha, beta)
        
        # Null Move Pruning (kh√¥ng d√πng trong PV node)
        if not is_pv and depth >= 3 and not self._in_check(board, player):
            null_score = -self._pvs_search(
                board, self._opponent(player), 
                depth - 3, -beta, -beta + 1, False
            )
            if null_score >= beta:
                return beta  # Null move cutoff
        
        # Generate and order moves
        moves = self._generate_ordered_moves(board, player, depth)
        
        best_score = -float('inf')
        best_move = None
        moves_searched = 0
        
        for move in moves:
            board.make_move(move, player)
            
            # PVS: First move searched with full window
            if moves_searched == 0:
                score = -self._pvs_search(
                    board, self._opponent(player),
                    depth - 1, -beta, -alpha, is_pv
                )
            else:
                # Late Move Reduction
                reduction = self._get_lmr_reduction(depth, moves_searched, is_pv)
                
                # Zero-window search
                score = -self._pvs_search(
                    board, self._opponent(player),
                    depth - 1 - reduction, -alpha - 1, -alpha, False
                )
                
                # Re-search if failed high
                if score > alpha and (is_pv or reduction > 0):
                    score = -self._pvs_search(
                        board, self._opponent(player),
                        depth - 1, -beta, -alpha, is_pv
                    )
            
            board.undo_move(move)
            moves_searched += 1
            
            if score > best_score:
                best_score = score
                best_move = move
                
                if score > alpha:
                    alpha = score
                    self.pv_table[depth] = move
                    
                    if score >= beta:
                        # Beta cutoff - update killer and history
                        self._update_killers(depth, move)
                        self._update_history(player, move, depth)
                        break
        
        # Store in TT
        self.tt.store(board.hash, depth, best_score, best_move, alpha, beta)
        
        return best_score
    
    def _quiescence_search(self, board, player, alpha, beta, depth=0):
        """
        Quiescence Search - ch·ªâ search c√°c n∆∞·ªõc tactical.
        Tr√°nh horizon effect.
        """
        stand_pat = self.evaluator.evaluate(board, player)
        
        if stand_pat >= beta:
            return beta
        if stand_pat > alpha:
            alpha = stand_pat
        
        # Ch·ªâ search c√°c n∆∞·ªõc t·∫°o threat ho·∫∑c block threat
        tactical_moves = self._generate_tactical_moves(board, player)
        
        for move in tactical_moves:
            board.make_move(move, player)
            score = -self._quiescence_search(
                board, self._opponent(player), -beta, -alpha, depth + 1
            )
            board.undo_move(move)
            
            if score >= beta:
                return beta
            if score > alpha:
                alpha = score
        
        return alpha
    
    def _generate_ordered_moves(self, board, player, depth):
        """
        Move ordering - critical cho pruning efficiency.
        
        Order:
        1. Hash move (from TT)
        2. Winning captures (VCF moves)
        3. Killer moves
        4. Countermoves
        5. History heuristic
        6. Remaining moves by static eval
        """
        moves = []
        
        # 1. Hash move
        tt_move = self.tt.get_best_move(board.hash)
        if tt_move:
            moves.append((tt_move, 10000000))
        
        # 2. VCF/Tactical moves
        vcf_moves = self._find_vcf_moves(board, player)
        for m in vcf_moves:
            if m != tt_move:
                moves.append((m, 5000000))
        
        # 3. Killer moves
        for killer in self.killer_moves[depth]:
            if killer and self._is_valid_move(board, killer):
                moves.append((killer, 1000000))
        
        # 4. All other moves with history score
        all_moves = self._generate_all_moves(board)
        for m in all_moves:
            if m not in [x[0] for x in moves]:
                history_score = self.history.get((player, m), 0)
                static_score = self._static_move_score(board, m, player)
                moves.append((m, history_score + static_score))
        
        # Sort by score descending
        moves.sort(key=lambda x: x[1], reverse=True)
        return [m[0] for m in moves]
    
    def _get_lmr_reduction(self, depth, moves_searched, is_pv):
        """
        Late Move Reduction table.
        Reduce search depth for moves that are unlikely to be good.
        """
        if depth < 3 or moves_searched < 4:
            return 0
        
        # LMR reduction table (pre-computed)
        reduction = int(0.5 + math.log(depth) * math.log(moves_searched) / 2.0)
        
        # Less reduction in PV nodes
        if is_pv:
            reduction = max(0, reduction - 1)
        
        return min(reduction, depth - 1)
```

### Task 1.2: Zobrist Hashing
**Priority: HIGH | Impact: +50% TT efficiency**

```python
# ai/analysis/zobrist.py

import random

class ZobristHash:
    """
    Zobrist hashing cho board positions.
    Cho ph√©p incremental update khi make/undo move.
    """
    
    def __init__(self, board_size=15, seed=42):
        random.seed(seed)
        self.board_size = board_size
        
        # Random numbers cho m·ªói (position, piece)
        self.piece_keys = {}
        for x in range(board_size):
            for y in range(board_size):
                for piece in ['X', 'O']:
                    self.piece_keys[(x, y, piece)] = random.getrandbits(64)
        
        # Side to move
        self.side_key = random.getrandbits(64)
    
    def compute_hash(self, board, side_to_move):
        """Compute full hash t·ª´ board state."""
        h = 0
        for x in range(self.board_size):
            for y in range(self.board_size):
                piece = board[x][y]
                if piece:
                    h ^= self.piece_keys[(x, y, piece)]
        
        if side_to_move == 'O':
            h ^= self.side_key
        
        return h
    
    def update_hash(self, current_hash, x, y, piece, side_changed=True):
        """Incremental hash update khi make/undo move."""
        h = current_hash ^ self.piece_keys[(x, y, piece)]
        if side_changed:
            h ^= self.side_key
        return h
```

### Task 1.3: Enhanced Transposition Table
**Priority: HIGH | Impact: +40% search speed**

```python
# ai/analysis/transposition_table_v2.py

from dataclasses import dataclass
from enum import Enum
import numpy as np

class TTEntryType(Enum):
    EXACT = 0      # Exact score
    LOWER = 1      # Score is lower bound (beta cutoff)
    UPPER = 2      # Score is upper bound (failed low)

@dataclass
class TTEntry:
    hash_key: int
    depth: int
    score: float
    entry_type: TTEntryType
    best_move: tuple
    age: int

class TranspositionTableV2:
    """
    Professional-grade Transposition Table.
    
    Features:
    - Replacement strategy: Depth-preferred v·ªõi age consideration
    - Two-tier structure: Always-replace + Depth-preferred
    - Prefetch support
    - Lock-free design cho multi-threading
    """
    
    def __init__(self, size_mb=128):
        # Calculate number of entries
        entry_size = 32  # bytes per entry
        self.num_entries = (size_mb * 1024 * 1024) // entry_size
        
        # Two-tier table
        self.always_replace = [None] * self.num_entries
        self.depth_preferred = [None] * self.num_entries
        
        self.current_age = 0
        self.hits = 0
        self.misses = 0
    
    def new_search(self):
        """Call at start of each new search."""
        self.current_age += 1
    
    def probe(self, hash_key, depth, alpha, beta):
        """
        Probe TT for a position.
        Returns (score, best_move) if usable, None otherwise.
        """
        index = hash_key % self.num_entries
        
        # Check both tiers
        for entry in [self.depth_preferred[index], self.always_replace[index]]:
            if entry and entry.hash_key == hash_key:
                self.hits += 1
                
                # Check if entry is usable at this depth
                if entry.depth >= depth:
                    if entry.entry_type == TTEntryType.EXACT:
                        return entry.score, entry.best_move
                    elif entry.entry_type == TTEntryType.LOWER and entry.score >= beta:
                        return entry.score, entry.best_move
                    elif entry.entry_type == TTEntryType.UPPER and entry.score <= alpha:
                        return entry.score, entry.best_move
                
                # Return best move even if score not usable
                return None, entry.best_move
        
        self.misses += 1
        return None, None
    
    def store(self, hash_key, depth, score, best_move, alpha, beta):
        """Store position in TT."""
        index = hash_key % self.num_entries
        
        # Determine entry type
        if score <= alpha:
            entry_type = TTEntryType.UPPER
        elif score >= beta:
            entry_type = TTEntryType.LOWER
        else:
            entry_type = TTEntryType.EXACT
        
        new_entry = TTEntry(
            hash_key=hash_key,
            depth=depth,
            score=score,
            entry_type=entry_type,
            best_move=best_move,
            age=self.current_age
        )
        
        # Always-replace tier
        self.always_replace[index] = new_entry
        
        # Depth-preferred tier - only replace if deeper or older
        existing = self.depth_preferred[index]
        if (not existing or 
            depth > existing.depth or 
            self.current_age - existing.age > 2):
            self.depth_preferred[index] = new_entry
```

### Task 1.4: Threat Space Search (TSS)
**Priority: HIGH | Impact: +30% tactical accuracy**

```python
# ai/analysis/threat_space_search.py

class ThreatSpaceSearch:
    """
    Threat Space Search - specialized search cho Gomoku.
    
    Thay v√¨ search t·∫•t c·∫£ moves, TSS ch·ªâ search trong "threat space":
    - Moves t·∫°o threat
    - Moves block threat
    - Moves extend existing threats
    
    ƒê√¢y l√† k·ªπ thu·∫≠t ƒë∆∞·ª£c d√πng b·ªüi c√°c engine m·∫°nh nh·∫•t.
    """
    
    def __init__(self, threat_detector, board_size=15):
        self.threat_detector = threat_detector
        self.board_size = board_size
        self.max_depth = 30  # TSS c√≥ th·ªÉ search r·∫•t s√¢u
    
    def search(self, board, player, depth=20):
        """
        Search trong threat space.
        
        Returns:
            (found_win, winning_sequence, score)
        """
        return self._tss_search(board, player, depth, [], set())
    
    def _tss_search(self, board, player, depth, sequence, visited):
        """
        Recursive TSS.
        
        Key insight: Trong Gomoku, n·∫øu kh√¥ng c√≥ threat n√†o,
        position th∆∞·ªùng l√† draw ho·∫∑c c·∫ßn positional play.
        """
        if depth == 0:
            return False, [], 0
        
        opponent = 'O' if player == 'X' else 'X'
        
        # Check immediate win
        player_threats = self.threat_detector.detect_all_threats(board, player)
        if player_threats.threats.get(ThreatType.FIVE, 0) > 0:
            return True, sequence, 100000
        
        # Check if opponent has winning threat
        opp_threats = self.threat_detector.detect_all_threats(board, opponent)
        if opp_threats.threats.get(ThreatType.FIVE, 0) > 0:
            return False, [], -100000
        
        # Generate threat moves only
        threat_moves = self._generate_threat_moves(board, player, player_threats, opp_threats)
        
        if not threat_moves:
            # No threats - evaluate position
            return False, [], self._evaluate_position(board, player)
        
        best_result = (False, [], -float('inf'))
        
        for move, move_type in threat_moves:
            # Make move
            board[move[0]][move[1]] = player
            new_sequence = sequence + [(move, player)]
            
            # Hash for visited check
            board_hash = self._hash_board(board)
            if board_hash in visited:
                board[move[0]][move[1]] = None
                continue
            visited.add(board_hash)
            
            # If this creates OPEN_FOUR, it's a win
            if move_type == 'open_four':
                board[move[0]][move[1]] = None
                return True, new_sequence, 100000
            
            # If this creates FOUR, opponent must respond
            if move_type == 'four':
                # Find blocking moves
                block_moves = self._find_blocking_moves(board, move, opponent)
                
                if len(block_moves) == 0:
                    # No block = win
                    board[move[0]][move[1]] = None
                    return True, new_sequence, 100000
                
                if len(block_moves) == 1:
                    # Single forced response - continue search
                    block = block_moves[0]
                    board[block[0]][block[1]] = opponent
                    
                    result = self._tss_search(
                        board, player, depth - 2, 
                        new_sequence + [(block, opponent)], visited
                    )
                    
                    board[block[0]][block[1]] = None
                    
                    if result[0]:  # Found win
                        board[move[0]][move[1]] = None
                        return result
                else:
                    # Multiple blocks - opponent can choose
                    # This branch is weaker
                    pass
            
            board[move[0]][move[1]] = None
            
            if best_result[2] < 0:
                best_result = (False, new_sequence, 0)
        
        return best_result
    
    def _generate_threat_moves(self, board, player, player_threats, opp_threats):
        """
        Generate moves trong threat space.
        
        Priority:
        1. Winning moves (create FIVE)
        2. Create OPEN_FOUR
        3. Create FOUR
        4. Block opponent's OPEN_FOUR
        5. Block opponent's FOUR
        6. Create OPEN_THREE
        7. Block opponent's OPEN_THREE
        """
        moves = []
        
        # 1. Winning moves
        for pos in self._find_winning_moves(board, player):
            moves.append((pos, 'five'))
        
        if moves:
            return moves  # If can win, just win
        
        # 2. Block opponent's winning moves
        for pos in self._find_winning_moves(board, 'O' if player == 'X' else 'X'):
            moves.append((pos, 'block_five'))
        
        if moves:
            return moves  # Must block
        
        # 3. Create OPEN_FOUR
        for pos in self._find_open_four_moves(board, player):
            moves.append((pos, 'open_four'))
        
        # 4. Create FOUR
        for pos in self._find_four_moves(board, player):
            moves.append((pos, 'four'))
        
        # 5. Block opponent's OPEN_FOUR
        # ... etc
        
        return moves
```

---

# üß† PHASE 2: NEURAL NETWORK EVALUATION (Tu·∫ßn 5-8)

### Task 1.1: Extended Pattern Detection
**Priority: HIGH | Impact: +15% accuracy**

```python
# Th√™m c√°c pattern m·ªõi v√†o threat_detector.py

class ThreatType(Enum):
    # Existing
    FIVE = "five"
    OPEN_FOUR = "open_four"
    FOUR = "four"
    # ...
    
    # NEW PATTERNS
    OVERLINE = "overline"           # 6+ li√™n ti·∫øp (c·∫•m trong Renju)
    DOUBLE_BROKEN_THREE = "double_broken_three"  # X_XX + XX_X c√πng ƒëi·ªÉm
    POTENTIAL_FOUR = "potential_four"  # 3 qu√¢n + 2 √¥ tr·ªëng li√™n ti·∫øp
    POTENTIAL_THREE = "potential_three"  # 2 qu√¢n + 3 √¥ tr·ªëng li√™n ti·∫øp
```

**Implementation:**
- [ ] Th√™m `detect_overline()` method
- [ ] Th√™m `detect_potential_patterns()` method
- [ ] C·∫≠p nh·∫≠t `_find_broken_patterns_in_line()` ƒë·ªÉ cover th√™m case
- [ ] Unit tests cho c√°c pattern m·ªõi

### Task 1.2: Influence Map
**Priority: MEDIUM | Impact: +10% evaluation accuracy**

```python
# Th√™m v√†o position_evaluator.py ho·∫∑c t·∫°o file m·ªõi

class InfluenceMap:
    """
    T√≠nh to√°n v√πng ·∫£nh h∆∞·ªüng c·ªßa m·ªói qu√¢n c·ªù.
    M·ªói √¥ tr·ªëng c√≥ influence score d·ª±a tr√™n:
    - Kho·∫£ng c√°ch ƒë·∫øn qu√¢n c·ªù g·∫ßn nh·∫•t
    - S·ªë qu√¢n c·ªù trong b√°n k√≠nh
    - H∆∞·ªõng ph√°t tri·ªÉn ti·ªÅm nƒÉng
    """
    
    def calculate_influence(self, board, player) -> List[List[float]]:
        """Return 15x15 influence map"""
        pass
    
    def get_contested_zones(self, board) -> List[Tuple[int, int]]:
        """T√¨m c√°c v√πng ƒëang tranh ch·∫•p"""
        pass
```

**Implementation:**
- [ ] T·∫°o `ai/analysis/influence_map.py`
- [ ] Integrate v√†o AdvancedEvaluator
- [ ] Property tests cho influence calculation

### Task 1.3: Threat Urgency Scoring
**Priority: HIGH | Impact: +12% defense accuracy**

```python
# C·∫£i thi·ªán threat scoring d·ª±a tr√™n urgency

THREAT_URGENCY = {
    ThreatType.FIVE: 10,           # ƒê√£ th·∫Øng
    ThreatType.OPEN_FOUR: 9,       # Ph·∫£i ch·∫∑n ngay
    ThreatType.DOUBLE_FOUR: 9,     # Kh√¥ng th·ªÉ ch·∫∑n
    ThreatType.FOUR_THREE: 8,      # R·∫•t nguy hi·ªÉm
    ThreatType.FOUR: 7,            # Ph·∫£i ch·∫∑n
    ThreatType.DOUBLE_THREE: 6,    # Nguy hi·ªÉm
    ThreatType.OPEN_THREE: 5,      # C·∫ßn ch√∫ √Ω
    ThreatType.BROKEN_FOUR: 5,     # Ti·ªÅm ·∫©n
    # ...
}

def calculate_urgency_score(threats: ThreatResult) -> float:
    """T√≠nh ƒëi·ªÉm urgency t·ªïng h·ª£p"""
    pass
```

---

## Phase 2: Position Evaluation Improvement (Tu·∫ßn 3-4)

### Task 2.1: Multi-Factor Weight Tuning
**Priority: HIGH | Impact: +20% evaluation accuracy**

Hi·ªán t·∫°i weights trong `AdvancedEvaluator`:
```python
THREAT_WEIGHT = 1.0
SHAPE_WEIGHT = 0.15
CONNECTIVITY_WEIGHT = 0.1
TERRITORY_WEIGHT = 0.08
TEMPO_WEIGHT = 0.12
```

**C·∫ßn tune l·∫°i d·ª±a tr√™n:**
- [ ] Ph√¢n t√≠ch 1000+ v√°n ƒë·∫•u pro
- [ ] A/B testing v·ªõi c√°c weight kh√°c nhau
- [ ] Machine learning ƒë·ªÉ t√¨m optimal weights

**Proposed new weights:**
```python
THREAT_WEIGHT = 1.0
SHAPE_WEIGHT = 0.20       # TƒÉng t·ª´ 0.15
CONNECTIVITY_WEIGHT = 0.15 # TƒÉng t·ª´ 0.10
TERRITORY_WEIGHT = 0.12   # TƒÉng t·ª´ 0.08
TEMPO_WEIGHT = 0.18       # TƒÉng t·ª´ 0.12
POTENTIAL_WEIGHT = 0.10   # M·ªöI
```

### Task 2.2: Potential Score Calculation
**Priority: MEDIUM | Impact: +15% prediction accuracy**

```python
def calculate_potential_score(self, board, player) -> float:
    """
    ƒê√°nh gi√° ti·ªÅm nƒÉng ph√°t tri·ªÉn c·ªßa v·ªã tr√≠.
    
    Factors:
    - S·ªë ƒë∆∞·ªùng c√≥ th·ªÉ t·∫°o 5 li√™n ti·∫øp
    - S·ªë √¥ tr·ªëng c√≥ th·ªÉ ph√°t tri·ªÉn
    - Flexibility (s·ªë h∆∞·ªõng c√≥ th·ªÉ ƒëi)
    """
    pass
```

### Task 2.3: Dynamic Evaluation Based on Game Phase
**Priority: MEDIUM | Impact: +10% accuracy**

```python
class GamePhase(Enum):
    OPENING = "opening"      # Move 1-10
    EARLY_MIDDLE = "early_middle"  # Move 11-25
    MIDDLE = "middle"        # Move 26-50
    LATE_MIDDLE = "late_middle"    # Move 51-80
    ENDGAME = "endgame"      # Move 81+

def get_phase_weights(phase: GamePhase) -> Dict[str, float]:
    """
    Tr·∫£ v·ªÅ weights ph√π h·ª£p v·ªõi phase c·ªßa game.
    
    Opening: Territory > Potential > Threat
    Middle: Threat > Shape > Connectivity
    Endgame: Threat > Tempo > Everything else
    """
    pass
```

---

## Phase 3: Mistake Analysis Enhancement (Tu·∫ßn 5-6)

### Task 3.1: Complete Mistake Categorization
**Priority: HIGH | Impact: +25% educational value**

Ho√†n thi·ªán `MistakeAnalyzer` v·ªõi ƒë·∫ßy ƒë·ªß categories:

```python
class MistakeCategory(Enum):
    # Tactical Mistakes
    MISSED_WIN = "missed_win"           # B·ªè l·ª° th·∫Øng
    MISSED_THREAT = "missed_threat"     # B·ªè l·ª° t·∫°o threat
    FAILED_DEFENSE = "failed_defense"   # Kh√¥ng ch·∫∑n threat
    
    # Positional Mistakes
    POOR_POSITION = "poor_position"     # V·ªã tr√≠ k√©m
    EDGE_PLAY = "edge_play"             # Ch∆°i r√¨a khi kh√¥ng c·∫ßn
    OVERCONCENTRATION = "overconcentration"  # T·∫≠p trung qu√° nhi·ªÅu 1 v√πng
    
    # Strategic Mistakes
    WRONG_DIRECTION = "wrong_direction"  # Sai h∆∞·ªõng ph√°t tri·ªÉn
    IGNORED_WEAKNESS = "ignored_weakness"  # B·ªè qua ƒëi·ªÉm y·∫øu
    
    # Tempo Mistakes
    SLOW_MOVE = "slow_move"             # N∆∞·ªõc ƒëi ch·∫≠m
    PASSIVE_PLAY = "passive_play"       # Ch∆°i th·ª• ƒë·ªông
    LOST_INITIATIVE = "lost_initiative"  # M·∫•t quy·ªÅn ch·ªß ƒë·ªông
```

### Task 3.2: Context-Aware Explanations
**Priority: MEDIUM | Impact: +20% understanding**

```python
def generate_contextual_explanation(
    self,
    mistake: CategorizedMistake,
    game_context: GameContext
) -> str:
    """
    T·∫°o explanation d·ª±a tr√™n context c·ªßa game.
    
    V√≠ d·ª•:
    - "·ªû n∆∞·ªõc 15, b·∫°n ƒë√£ b·ªè l·ª° c∆° h·ªôi t·∫°o T·ª© Tam t·∫°i H8. 
       ƒê√¢y l√† th·ªùi ƒëi·ªÉm quan tr·ªçng v√¨ ƒë·ªëi th·ªß ƒëang y·∫øu ·ªü c√°nh ph·∫£i."
    """
    pass
```

### Task 3.3: Educational Tips Generation
**Priority: MEDIUM | Impact: +15% learning**

```python
EDUCATIONAL_TIPS = {
    MistakeCategory.MISSED_WIN: [
        "Lu√¥n ki·ªÉm tra xem c√≥ VCF (Victory by Continuous Four) kh√¥ng tr∆∞·ªõc khi ƒëi.",
        "Khi c√≥ 3 qu√¢n li√™n ti·∫øp v·ªõi 2 ƒë·∫ßu m·ªü, h√£y t√¨m c√°ch t·∫°o T·ª© Tam.",
    ],
    MistakeCategory.FAILED_DEFENSE: [
        "Khi ƒë·ªëi th·ªß c√≥ 3 qu√¢n m·ªü, ∆∞u ti√™n ch·∫∑n tr∆∞·ªõc khi t·∫•n c√¥ng.",
        "ƒê·∫øm s·ªë threat c·ªßa ƒë·ªëi th·ªß tr∆∞·ªõc m·ªói n∆∞·ªõc ƒëi.",
    ],
    # ...
}
```

---

## Phase 4: Pattern Recognition Expansion (Tu·∫ßn 7-8)

### Task 4.1: Advanced Pattern Library
**Priority: HIGH | Impact: +30% pattern coverage**

```python
# Th√™m c√°c pattern m·ªõi

ADVANCED_PATTERNS = {
    "fork": {
        "label": "R·∫Ω Nh√°nh",
        "explanation": "T·∫°o 2 ƒë∆∞·ªùng t·∫•n c√¥ng t·ª´ 1 n∆∞·ªõc ƒëi",
        "detection": detect_fork_pattern,
    },
    "ladder": {
        "label": "Thang",
        "explanation": "Chu·ªói n∆∞·ªõc ƒëi bu·ªôc ƒë·ªëi th·ªß ph·∫£i theo",
        "detection": detect_ladder_pattern,
    },
    "sacrifice": {
        "label": "Hy Sinh",
        "explanation": "B·ªè 1 ƒë∆∞·ªùng ƒë·ªÉ t·∫°o ƒë∆∞·ªùng m·∫°nh h∆°n",
        "detection": detect_sacrifice_pattern,
    },
    "trap": {
        "label": "B·∫´y",
        "explanation": "D·ª• ƒë·ªëi th·ªß v√†o v·ªã tr√≠ b·∫•t l·ª£i",
        "detection": detect_trap_pattern,
    },
    "connection": {
        "label": "K·∫øt N·ªëi",
        "explanation": "N·ªëi 2 nh√≥m qu√¢n ƒë·ªÉ tƒÉng s·ª©c m·∫°nh",
        "detection": detect_connection_pattern,
    },
    "cut": {
        "label": "C·∫Øt",
        "explanation": "Chia c·∫Øt qu√¢n ƒë·ªëi th·ªß",
        "detection": detect_cut_pattern,
    },
}
```

### Task 4.2: Pattern Sequence Detection
**Priority: MEDIUM | Impact: +15% tactical understanding**

```python
def detect_pattern_sequence(
    self,
    timeline: List[TimelineEntry],
    moves: List[Move]
) -> List[PatternSequence]:
    """
    Ph√°t hi·ªán chu·ªói pattern li√™n ti·∫øp.
    
    V√≠ d·ª•:
    - Fork ‚Üí Double Threat ‚Üí Win
    - Sacrifice ‚Üí Ladder ‚Üí Fork ‚Üí Win
    """
    pass
```

---

## Phase 5: Search Enhancement (Tu·∫ßn 9-10)

### Task 5.1: Iterative Deepening
**Priority: HIGH | Impact: +25% best move accuracy**

```python
def find_best_moves_iterative(
    self,
    board: List[List[Optional[str]]],
    player: str,
    max_depth: int = 6,
    time_limit_ms: int = 1500
) -> List[Tuple[int, int, float]]:
    """
    T√¨m best moves v·ªõi iterative deepening.
    
    1. Search depth 1, l∆∞u k·∫øt qu·∫£
    2. Search depth 2, c·∫≠p nh·∫≠t n·∫øu t·ªët h∆°n
    3. Ti·∫øp t·ª•c cho ƒë·∫øn khi h·∫øt time ho·∫∑c max_depth
    """
    pass
```

### Task 5.2: Move Ordering Optimization
**Priority: HIGH | Impact: +20% search efficiency**

```python
def order_moves(
    self,
    board: List[List[Optional[str]]],
    player: str,
    moves: List[Tuple[int, int]]
) -> List[Tuple[int, int]]:
    """
    S·∫Øp x·∫øp moves theo th·ª© t·ª± ∆∞u ti√™n:
    
    1. Killer moves (moves ƒë√£ g√¢y cutoff ·ªü depth tr∆∞·ªõc)
    2. Threat-creating moves
    3. Defensive moves
    4. Center-biased moves
    5. History heuristic
    """
    pass
```

### Task 5.3: Transposition Table Enhancement
**Priority: MEDIUM | Impact: +15% search speed**

```python
# C·∫£i thi·ªán transposition_table.py

class EnhancedTranspositionTable:
    """
    Transposition table v·ªõi:
    - Zobrist hashing
    - Replacement strategy (depth-preferred)
    - Age-based cleanup
    """
    
    def __init__(self, size_mb: int = 64):
        self.size = size_mb * 1024 * 1024 // 32  # 32 bytes per entry
        self.table = {}
        self.age = 0
    
    def store(self, hash_key: int, depth: int, score: float, 
              flag: EntryType, best_move: Tuple[int, int]):
        pass
    
    def probe(self, hash_key: int, depth: int, alpha: float, beta: float):
        pass
```

---

## Phase 6: Performance Optimization (Tu·∫ßn 11-12)

### Task 6.1: Numba JIT Compilation
**Priority: MEDIUM | Impact: +50% speed**

```python
# S·ª≠ d·ª•ng numba cho c√°c hot paths

from numba import jit, njit

@njit
def fast_threat_scan(board_array, player_id, direction):
    """Numba-optimized threat scanning"""
    pass

@njit
def fast_position_eval(board_array, player_id):
    """Numba-optimized position evaluation"""
    pass
```

### Task 6.2: Parallel Search
**Priority: LOW | Impact: +30% speed (multi-core)**

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_search(
    self,
    board: List[List[Optional[str]]],
    player: str,
    moves: List[Tuple[int, int]],
    depth: int
) -> List[Tuple[int, int, float]]:
    """
    Search song song cho c√°c root moves.
    """
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(self._search_move, board, player, move, depth)
            for move in moves
        ]
        results = [f.result() for f in futures]
    return results
```

### Task 6.3: Caching Strategy
**Priority: MEDIUM | Impact: +40% repeated analysis speed**

```python
# C·∫£i thi·ªán caching

class AnalysisCache:
    """
    Multi-level cache:
    - L1: In-memory LRU cache (hot data)
    - L2: Redis cache (warm data)
    - L3: Database cache (cold data)
    """
    
    def get(self, match_id: str, tier: str) -> Optional[AnalysisResult]:
        # Try L1 first
        # Then L2
        # Then L3
        pass
    
    def set(self, match_id: str, tier: str, result: AnalysisResult):
        # Store in all levels with appropriate TTL
        pass
```

---

## Metrics & Success Criteria

### Target Metrics (sau 12 tu·∫ßn)

| Metric | Hi·ªán t·∫°i | Target | Improvement |
|--------|----------|--------|-------------|
| Threat Detection Accuracy | 70% | 90% | +20% |
| Position Evaluation Accuracy | 60% | 80% | +20% |
| Mistake Detection Accuracy | 50% | 75% | +25% |
| Pattern Coverage | 40% | 80% | +40% |
| Best Move Accuracy | 55% | 80% | +25% |
| Analysis Speed (avg) | 1.5s | 1.0s | -33% |

### Testing Requirements

- [ ] Property-based tests cho m·ªói module m·ªõi
- [ ] Benchmark tests v·ªõi 100+ positions
- [ ] Regression tests ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng break existing functionality
- [ ] Integration tests cho full analysis pipeline

### Rollout Plan

1. **Week 1-2**: Phase 1 (Threat Detection) ‚Üí Deploy to staging
2. **Week 3-4**: Phase 2 (Position Evaluation) ‚Üí A/B test
3. **Week 5-6**: Phase 3 (Mistake Analysis) ‚Üí Deploy to 10% users
4. **Week 7-8**: Phase 4 (Pattern Recognition) ‚Üí Deploy to 50% users
5. **Week 9-10**: Phase 5 (Search Enhancement) ‚Üí Deploy to all
6. **Week 11-12**: Phase 6 (Performance) ‚Üí Optimize based on metrics

---

## Quick Wins (C√≥ th·ªÉ l√†m ngay)

### 1. Tune Existing Weights (1-2 ng√†y)
```python
# Trong advanced_evaluator.py
THREAT_WEIGHT = 1.0
SHAPE_WEIGHT = 0.20       # TƒÉng t·ª´ 0.15
CONNECTIVITY_WEIGHT = 0.15 # TƒÉng t·ª´ 0.10
TERRITORY_WEIGHT = 0.12   # TƒÉng t·ª´ 0.08
TEMPO_WEIGHT = 0.18       # TƒÉng t·ª´ 0.12
```

### 2. Improve Early Game Tolerance (1 ng√†y)
```python
# Trong basic_analyzer.py, tƒÉng early game tolerance
if move_number <= 10:  # TƒÉng t·ª´ 8
    # More lenient classification
```

### 3. Add More Vietnamese Notes (1 ng√†y)
```python
# Th√™m templates cho c√°c case m·ªõi
NOTE_TEMPLATES = {
    # ... existing
    MoveClassification.DEFENSIVE: [
        "N∆∞·ªõc ph√≤ng th·ªß t·ªët. {reason}",
        "Ch·∫∑n ƒë√∫ng l√∫c. {reason}",
    ],
    MoveClassification.DEVELOPING: [
        "Ph√°t tri·ªÉn h·ª£p l√Ω. {reason}",
        "X√¢y d·ª±ng th·∫ø tr·∫≠n. {reason}",
    ],
}
```

### 4. Better Mistake Descriptions (2 ng√†y)
```python
# C·∫£i thi·ªán _generate_mistake_description()
def _generate_mistake_description(self, move, severity, best_alt, actual_score, best_score):
    score_diff = best_score - actual_score
    
    if severity == "critical":
        if score_diff > 10000:
            return f"B·ªè l·ª° th·∫Øng! N√™n ƒëi ({best_alt[0]}, {best_alt[1]}) ƒë·ªÉ t·∫°o VCF."
        else:
            return f"Sai l·∫ßm nghi√™m tr·ªçng! ƒê·ªÉ ƒë·ªëi th·ªß c√≥ c∆° h·ªôi th·∫Øng."
    # ...
```

---

## K·∫øt Lu·∫≠n

Plan n√†y chia th√†nh 6 phases v·ªõi t·ªïng th·ªùi gian ~12 tu·∫ßn. M·ªói phase c√≥ th·ªÉ deploy ƒë·ªôc l·∫≠p v√† c√≥ metrics r√µ r√†ng ƒë·ªÉ ƒëo l∆∞·ªùng improvement.

**∆Øu ti√™n cao nh·∫•t:**
1. Phase 1: Threat Detection Enhancement
2. Phase 3: Mistake Analysis Enhancement
3. Phase 5: Search Enhancement

**Quick wins c√≥ th·ªÉ l√†m ngay trong 1 tu·∫ßn ƒë·∫ßu** ƒë·ªÉ th·∫•y improvement nhanh ch√≥ng.
